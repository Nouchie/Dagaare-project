{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SJjAcjawiVO_O1mLdbKoHMUz5AMjLa5A","authorship_tag":"ABX9TyMNlLHeDyrHIaJ2N5MFXKvB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -U \"transformers==4.38.1\" --upgrade\n","!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"IwEd5eQ12n7b","executionInfo":{"status":"ok","timestamp":1716214243306,"user_tz":0,"elapsed":142868,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"851e62c9-a11d-452f-cc40-81aeda26fcfd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.38.1\n","  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (2.31.0)\n","Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.1)\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1) (2024.2.2)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.2\n","    Uninstalling transformers-4.40.2:\n","      Successfully uninstalled transformers-4.40.2\n","Successfully installed tokenizers-0.15.2 transformers-4.38.1\n","Collecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Looking in indexes: https://pypi.org/simple/\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Installing collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.43.1\n"]}]},{"cell_type":"code","source":["!pip install accelerate"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"t5m3ltnm6eF2","executionInfo":{"status":"ok","timestamp":1716214250159,"user_tz":0,"elapsed":7006,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"ccd4d174-38df-4c15-db8a-2e55662a11be"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","userdata.get('HF_TOKEN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"chYpEXV_8UcU","executionInfo":{"status":"ok","timestamp":1716217663238,"user_tz":0,"elapsed":3609,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"16c0054b-5052-48de-9062-9cd2933d1e3a"},"execution_count":168,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'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'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":168}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","english_file = \"/content/drive/MyDrive/dagaare_words_definition/dagaare_definition_sentences.txt\"\n","dagaare_file = \"/content/drive/MyDrive/dagaare_words_definition/dagaare_words_sentences.txt\""],"metadata":{"id":"FKZ2YqpneffR","executionInfo":{"status":"ok","timestamp":1716217663241,"user_tz":0,"elapsed":31,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":169,"outputs":[]},{"cell_type":"code","source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-11B\", trust_remote_code=True)"],"metadata":{"id":"MW5mJdZIDTKe","executionInfo":{"status":"ok","timestamp":1716217663241,"user_tz":0,"elapsed":31,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":170,"outputs":[]},{"cell_type":"code","source":["START_TOKEN = '<START>'\n","PADDING_TOKEN = '<PADDING>'\n","END_TOKEN = '<END>'\n","\n","dagaare_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n","                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n","                      'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n","                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n","                        'Y', 'Z',\n","                        '[', '\\\\', ']', '^', '_', '`',\n","                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n","                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n","                        'y', 'z','à','á','â','ã','æ','è','é','ì','í','ò','ó','õ','ù','ú','û','ĩ','ŋ','ŏ','ũ',\n","                        'Ɔ','Ɛ','ǎ','ɔ','ɛ','ɡ','ɪ','ʊ','ʋ','ͻ','ε','ṹ','ạ','ẽ','ὸ','ό',\n","                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n","\n","english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n","                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n","                        ':', '<', '=', '>', '?', '@',\n","                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n","                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n","                        'Y', 'Z',\n","                        '[', '\\\\', ']', '^', '_', '`',\n","                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n","                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n","                        'y', 'z',\n","                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"],"metadata":{"id":"X5WH6PR8eX-i","executionInfo":{"status":"ok","timestamp":1716217663241,"user_tz":0,"elapsed":24,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":171,"outputs":[]},{"cell_type":"code","source":["text = \"sɛgebinyaŋaa\"\n","list(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Id4WgcoAkn4d","executionInfo":{"status":"ok","timestamp":1716217663241,"user_tz":0,"elapsed":23,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"692be330-df4a-45b4-a968-9de0fc330d7b"},"execution_count":172,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['s', 'ɛ', 'g', 'e', 'b', 'i', 'n', 'y', 'a', 'ŋ', 'a', 'a']"]},"metadata":{},"execution_count":172}]},{"cell_type":"code","source":["index_to_dagaare = {k:v for k,v in enumerate(dagaare_vocabulary)}\n","dagaare_to_index = {v:k for k,v in enumerate(dagaare_vocabulary)}\n","index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n","english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"],"metadata":{"id":"05NUAybRkn1B","executionInfo":{"status":"ok","timestamp":1716217663242,"user_tz":0,"elapsed":22,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","# Function to remove punctuation marks and special characters from text\n","def remove_special_characters(text):\n","    special_chars = ['“','́','…','\\t','–', '”','•', '’','‘','æ','ɪ','ạ', 'à', 'á', 'â', 'ã', 'è', 'é', 'ì', 'í', 'ò', 'ó', 'õ', 'ù', 'ú', 'û', 'ĩ', 'ŋ', 'ŏ', 'ũ', 'Ɔ', 'Ɛ', 'ǎ', 'ɔ', 'ɛ', 'ɡ', 'ʊ', 'ʋ', 'ͻ', 'ε', 'ṹ', 'ẽ', 'ὸ', 'ό']\n","    translator = str.maketrans('', '', string.punctuation)\n","    cleaned_text = text.translate(translator)\n","    for char in special_chars:\n","        cleaned_text = cleaned_text.replace(char, 'UNK')\n","    return cleaned_text\n","\n","# Read the contents of the English and Dagaare files\n","with open(english_file, 'r', encoding='utf-8') as f:\n","    english_sentences = f.readlines()\n","\n","with open(dagaare_file, 'r', encoding='utf-8') as f:\n","    dagaare_sentences = f.readlines()\n","\n","# Limit Number of sentences\n","TOTAL_SENTENCES = 500000\n","english_sentences = english_sentences[:TOTAL_SENTENCES]\n","dagaare_sentences = dagaare_sentences[:TOTAL_SENTENCES]\n","\n","# Remove special characters and strip newline characters from English and Dagaare sentences\n","english_sentences = [remove_special_characters(sentence.rstrip('\\n')) for sentence in english_sentences]\n","dagaare_sentences = [sentence.rstrip('\\n') for sentence in dagaare_sentences]"],"metadata":{"id":"8M19Ai4skny_","executionInfo":{"status":"ok","timestamp":1716217663242,"user_tz":0,"elapsed":21,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["english_sentences[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3l5qnLtknxJ","executionInfo":{"status":"ok","timestamp":1716217663242,"user_tz":0,"elapsed":21,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"e4648630-f64e-4f96-849f-b21bca98b410"},"execution_count":175,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Definition',\n"," '1 and',\n"," '2 in order to',\n"," 'but',\n"," 'they',\n"," 'the',\n"," 'to hate',\n"," 'expression of surprise and regret',\n"," 'expression of regret',\n"," 'expression expressing UNKI told you soUNK']"]},"metadata":{},"execution_count":175}]},{"cell_type":"code","source":["dagaare_sentences[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtKfd2CIknus","executionInfo":{"status":"ok","timestamp":1716217663243,"user_tz":0,"elapsed":20,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"6b23294f-02ea-47a1-9913-aa01152d12f9"},"execution_count":176,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Word', 'a', 'a', 'a', 'a', 'a', 'a', 'aa', 'aa', 'aa']"]},"metadata":{},"execution_count":176}]},{"cell_type":"code","source":["max(len(x) for x in dagaare_sentences), max(len(x) for x in english_sentences),"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fk59B0NfmPCE","executionInfo":{"status":"ok","timestamp":1716217663243,"user_tz":0,"elapsed":18,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"910b06cc-1d80-49e6-d7b4-b16631b782f3"},"execution_count":177,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(130, 373)"]},"metadata":{},"execution_count":177}]},{"cell_type":"code","source":["PERCENTILE = 97\n","print( f\"{PERCENTILE}th percentile length dagaare: {np.percentile([len(x) for x in dagaare_sentences], PERCENTILE)}\" )\n","print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ug7juppXmY-B","executionInfo":{"status":"ok","timestamp":1716217663243,"user_tz":0,"elapsed":17,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"953dec14-5d3d-4e14-b012-dad726ace361"},"execution_count":178,"outputs":[{"output_type":"stream","name":"stdout","text":["97th percentile length dagaare: 21.0\n","97th percentile length English: 73.0\n"]}]},{"cell_type":"code","source":["# Your existing code for checking valid sentences\n","max_sequence_length = 200\n","\n","def is_valid_tokens(sentence, vocab):\n","    for token in list(set(sentence)):\n","        if token not in vocab:\n","            return False\n","    return True\n","\n","def is_valid_length(sentence, max_sequence_length):\n","    return len(list(sentence)) < (max_sequence_length - 1)  # need to re-add the end token so leaving 1 space\n","\n","valid_sentence_indices = []\n","for index in range(len(dagaare_sentences)):\n","    dagaare_sentence, english_sentence = dagaare_sentences[index], english_sentences[index]\n","    if is_valid_length(dagaare_sentence, max_sequence_length) \\\n","       and is_valid_length(english_sentence, max_sequence_length) \\\n","       and is_valid_tokens(dagaare_sentence, dagaare_vocabulary):\n","        valid_sentence_indices.append(index)\n","\n","print(f\"Number of sentences: {len(dagaare_sentences)}\")\n","print(f\"Number of valid sentences: {len(valid_sentence_indices)}\")\n","\n","# Filter sentences based on valid indices\n","dagaare_sentences = [dagaare_sentences[i] for i in valid_sentence_indices]\n","english_sentences = [english_sentences[i] for i in valid_sentence_indices]\n","\n","print(f\"Number of sentences after filtering: {len(dagaare_sentences)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BujGE-pFmZ_Z","executionInfo":{"status":"ok","timestamp":1716217663727,"user_tz":0,"elapsed":496,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"544bf749-839e-4d11-874e-2ee5a46e4f5e"},"execution_count":179,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of sentences: 8112\n","Number of valid sentences: 7210\n","Number of sentences after filtering: 7210\n"]}]},{"cell_type":"code","source":["dagaare_sentences[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UH8kneLRmZ5f","executionInfo":{"status":"ok","timestamp":1716217663727,"user_tz":0,"elapsed":19,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"8909bc21-dd8d-454a-d946-de4a2ab513f8"},"execution_count":180,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Word', 'a', 'a']"]},"metadata":{},"execution_count":180}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class TextDataset(Dataset):\n","\n","    def __init__(self, english_sentences, dagaare_sentences):\n","        self.english_sentences = english_sentences\n","        self.dagaare_sentences = dagaare_sentences\n","\n","    def __len__(self):\n","        return len(self.english_sentences)\n","\n","    def __getitem__(self, idx):\n","        return self.english_sentences[idx], self.dagaare_sentences[idx]"],"metadata":{"id":"2pnueoZWmZ3a","executionInfo":{"status":"ok","timestamp":1716217663727,"user_tz":0,"elapsed":17,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":181,"outputs":[]},{"cell_type":"code","source":["dataset = TextDataset(english_sentences, dagaare_sentences)"],"metadata":{"id":"rO8Q7KRDmZ1M","executionInfo":{"status":"ok","timestamp":1716217663728,"user_tz":0,"elapsed":17,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":182,"outputs":[]},{"cell_type":"code","source":["len(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6i2ORnrmZzL","executionInfo":{"status":"ok","timestamp":1716217663728,"user_tz":0,"elapsed":17,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"412fabd9-9b8e-4ea0-9eea-d597a8a17dd8"},"execution_count":183,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7210"]},"metadata":{},"execution_count":183}]},{"cell_type":"code","source":["batch_size = 1\n","train_loader = DataLoader(dataset, batch_size)\n","iterator = iter(train_loader)"],"metadata":{"id":"J5VRdmCyntdc","executionInfo":{"status":"ok","timestamp":1716217663728,"user_tz":0,"elapsed":15,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":184,"outputs":[]},{"cell_type":"code","source":["for batch_num, batch in enumerate(iterator):\n","    print(batch)\n","    if batch_num > 1:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RP7EHlzgnta8","executionInfo":{"status":"ok","timestamp":1716217663728,"user_tz":0,"elapsed":15,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"84c65261-187b-4281-fcd3-207e0e862b74"},"execution_count":185,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Definition',), ('Word',)]\n","[('1 and',), ('a',)]\n","[('2 in order to',), ('a',)]\n"]}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-11B\", trust_remote_code=True)\n","\n","eng_tokenized, dg_tokenized = [], []\n","\n","for sentence_num in range(batch_size):\n","    eng_sentence, dg_sentence = batch[0][sentence_num], batch[1][sentence_num]\n","\n","    # Tokenize English sentence\n","    eng_tokens = tokenizer.encode(eng_sentence, add_special_tokens=False)\n","    eng_tokenized.append(eng_tokens)\n","\n","    # Tokenize Dagaare sentence\n","    dg_tokens = tokenizer.encode(dg_sentence, add_special_tokens=True)\n","    dg_tokenized.append(dg_tokens)\n","\n","# Pad sequences to the same length if necessary\n","max_length = max(len(tokens) for tokens in eng_tokenized + dg_tokenized)\n","eng_tokenized = [tokens + [tokenizer.pad_token_id] * (max_length - len(tokens)) for tokens in eng_tokenized]\n","dg_tokenized = [tokens + [tokenizer.pad_token_id] * (max_length - len(tokens)) for tokens in dg_tokenized]\n","\n","# Convert to torch tensors and stack\n","eng_tokenized = torch.tensor(eng_tokenized)\n","dg_tokenized = torch.tensor(dg_tokenized)"],"metadata":{"id":"6kvRBkxNoXig","executionInfo":{"status":"ok","timestamp":1716217663728,"user_tz":0,"elapsed":13,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":186,"outputs":[]},{"cell_type":"code","source":["batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vorCsyoyb1x","executionInfo":{"status":"ok","timestamp":1716217663728,"user_tz":0,"elapsed":12,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"20787c1d-23f7-4f8c-8d90-dbd308fed32c"},"execution_count":187,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('2 in order to',), ('a',)]"]},"metadata":{},"execution_count":187}]},{"cell_type":"code","source":["# Convert to torch tensors\n","eng_tokenized = eng_tokenized.clone().detach()\n","dg_tokenized = dg_tokenized.clone().detach()"],"metadata":{"id":"UO0J0PlI0V4V","executionInfo":{"status":"ok","timestamp":1716217663729,"user_tz":0,"elapsed":12,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":188,"outputs":[]},{"cell_type":"code","source":["eng_tokenized"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hM2F6h73oxs_","executionInfo":{"status":"ok","timestamp":1716217663729,"user_tz":0,"elapsed":11,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"448b5594-378c-4c0a-c328-3717425203ef"},"execution_count":189,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  29,  272, 1538,  271]])"]},"metadata":{},"execution_count":189}]},{"cell_type":"code","source":["dg_tokenized"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CWn3OEgoxq6","executionInfo":{"status":"ok","timestamp":1716217663729,"user_tz":0,"elapsed":10,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"0fe3ed9e-7e3d-4f71-d551-888a776d06fb"},"execution_count":190,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[76, 11, 11, 11]])"]},"metadata":{},"execution_count":190}]},{"cell_type":"code","source":["import torch\n","\n","NEG_INFTY = -1e9\n","\n","def create_masks(eng_batch, dg_batch, max_sequence_length):\n","    num_sentences = len(eng_batch)\n","\n","    # Look-ahead mask for decoder self-attention to prevent attending to future tokens\n","    look_ahead_mask = torch.triu(torch.ones(max_sequence_length, max_sequence_length), diagonal=1).bool()\n","\n","    # Padding masks for encoder and decoder\n","    encoder_padding_mask = torch.full((num_sentences, max_sequence_length, max_sequence_length), False, dtype=torch.bool)\n","    decoder_padding_mask_self_attention = torch.full((num_sentences, max_sequence_length, max_sequence_length), False, dtype=torch.bool)\n","    decoder_padding_mask_cross_attention = torch.full((num_sentences, max_sequence_length, max_sequence_length), False, dtype=torch.bool)\n","\n","    for idx in range(num_sentences):\n","        eng_sentence_length = len(eng_batch[idx])\n","        dg_sentence_length = len(dg_batch[idx])\n","\n","        # Padding masks for encoder\n","        if eng_sentence_length < max_sequence_length:\n","            eng_chars_to_padding_mask = torch.arange(eng_sentence_length, max_sequence_length)\n","            encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n","            encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n","\n","        # Padding masks for decoder self-attention\n","        if dg_sentence_length < max_sequence_length:\n","            dg_chars_to_padding_mask = torch.arange(dg_sentence_length, max_sequence_length)\n","            decoder_padding_mask_self_attention[idx, :, dg_chars_to_padding_mask] = True\n","            decoder_padding_mask_self_attention[idx, dg_chars_to_padding_mask, :] = True\n","\n","        # Padding masks for decoder cross-attention\n","        if eng_sentence_length < max_sequence_length:\n","            decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n","        if dg_sentence_length < max_sequence_length:\n","            decoder_padding_mask_cross_attention[idx, dg_chars_to_padding_mask, :] = True\n","\n","    # Combine padding masks with look-ahead mask for decoder self-attention\n","    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n","    decoder_self_attention_mask = torch.where(look_ahead_mask | decoder_padding_mask_self_attention, NEG_INFTY, 0)\n","    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n","\n","    # Print mask sizes and some sample values for debugging\n","    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :10, :10]}\")\n","    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :10, :10]}\")\n","    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :10, :10]}\")\n","\n","    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"],"metadata":{"id":"NaAyBukqoxo8","executionInfo":{"status":"ok","timestamp":1716217663729,"user_tz":0,"elapsed":8,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":191,"outputs":[]},{"cell_type":"markdown","source":["max_sequence_length = 100  # Example value, replace with actual sequence length\n","create_masks(batch[0], batch[1], max_sequence_length)"],"metadata":{"id":"C4276YJfoxm9"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/dagaare_words_definition/')"],"metadata":{"collapsed":true,"id":"P7sRgpTl1Hvh","executionInfo":{"status":"ok","timestamp":1716217663730,"user_tz":0,"elapsed":9,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":192,"outputs":[]},{"cell_type":"code","source":["import torch\n","from Transformer import Transformer"],"metadata":{"id":"Lxbima91yIt9","executionInfo":{"status":"ok","timestamp":1716217663730,"user_tz":0,"elapsed":8,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":193,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","d_model = 512\n","batch_size = 30\n","ffn_hidden = 2048\n","num_heads = 8\n","drop_prob = 0.1\n","num_layers = 1\n","max_sequence_length = 200\n","dg_vocab_size = len(dagaare_vocabulary)\n","\n","transformer = Transformer(d_model,\n","                          ffn_hidden,\n","                          num_heads,\n","                          drop_prob,\n","                          num_layers,\n","                          max_sequence_length,\n","                          dg_vocab_size,\n","                          english_to_index,\n","                          dagaare_to_index,\n","                          START_TOKEN,\n","                          END_TOKEN,\n","                          PADDING_TOKEN)"],"metadata":{"id":"s_8t7bWgoxlB","executionInfo":{"status":"ok","timestamp":1716217664428,"user_tz":0,"elapsed":706,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":194,"outputs":[]},{"cell_type":"code","source":["transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPRoFHQFoxiP","executionInfo":{"status":"ok","timestamp":1716217664428,"user_tz":0,"elapsed":21,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"ecb29c90-c482-4b5d-a545-411a39c7df2f"},"execution_count":195,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Transformer(\n","  (encoder): Encoder(\n","    (sentence_embedding): SentenceEmbedding(\n","      (embedding): Embedding(97, 512)\n","      (position_encoder): PositionalEncoding()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (layers): SequentialEncoder(\n","      (0): EncoderLayer(\n","        (attention): MultiHeadAttention(\n","          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n","          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (norm1): LayerNormalization()\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (ffn): PositionwiseFeedForward(\n","          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","          (relu): ReLU()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (norm2): LayerNormalization()\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (decoder): Decoder(\n","    (sentence_embedding): SentenceEmbedding(\n","      (embedding): Embedding(132, 512)\n","      (position_encoder): PositionalEncoding()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (layers): SequentialDecoder(\n","      (0): DecoderLayer(\n","        (self_attention): MultiHeadAttention(\n","          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n","          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (layer_norm1): LayerNormalization()\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (encoder_decoder_attention): MultiHeadCrossAttention(\n","          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n","          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n","          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (layer_norm2): LayerNormalization()\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (ffn): PositionwiseFeedForward(\n","          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","          (relu): ReLU()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm3): LayerNormalization()\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=132, bias=True)\n",")"]},"metadata":{},"execution_count":195}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class TextDataset(Dataset):\n","\n","    def __init__(self, english_sentences, dagaare_sentences):\n","        self.english_sentences = english_sentences\n","        self.dagaare_sentences = dagaare_sentences\n","\n","    def __len__(self):\n","        return len(self.english_sentences)\n","\n","    def __getitem__(self, idx):\n","        return self.english_sentences[idx], self.dagaare_sentences[idx]"],"metadata":{"id":"IruYPeXe32kL","executionInfo":{"status":"ok","timestamp":1716217664428,"user_tz":0,"elapsed":19,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":196,"outputs":[]},{"cell_type":"code","source":["dataset = TextDataset(english_sentences, dagaare_sentences)"],"metadata":{"id":"B88Kh-B64FH4","executionInfo":{"status":"ok","timestamp":1716217664428,"user_tz":0,"elapsed":19,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":197,"outputs":[]},{"cell_type":"code","source":["len(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8YHAjh14oub","executionInfo":{"status":"ok","timestamp":1716217664428,"user_tz":0,"elapsed":19,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"80d4347f-0494-4aba-f9cd-a3334a22f9f5"},"execution_count":198,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7210"]},"metadata":{},"execution_count":198}]},{"cell_type":"code","source":["dataset[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZWqgpMl4ruJ","executionInfo":{"status":"ok","timestamp":1716217664429,"user_tz":0,"elapsed":14,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"d719e62f-8687-439b-a817-4e7688cb016d"},"execution_count":199,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('1 and', 'a')"]},"metadata":{},"execution_count":199}]},{"cell_type":"code","source":["train_loader = DataLoader(dataset, batch_size)\n","iterator = iter(train_loader)"],"metadata":{"id":"g6hVENbN4vn_","executionInfo":{"status":"ok","timestamp":1716217664429,"user_tz":0,"elapsed":12,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":200,"outputs":[]},{"cell_type":"code","source":["for batch_num, batch in enumerate(iterator):\n","    print(batch)\n","    if batch_num > 3:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKH3jawN4yio","executionInfo":{"status":"ok","timestamp":1716217664429,"user_tz":0,"elapsed":12,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"8e8ce262-dfb9-4113-8472-52fb4813a729"},"execution_count":201,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Definition', '1 and', '2 in order to', 'but', 'they', 'the', 'to hate', 'expression of surprise and regret', 'expression of regret', 'expression expressing UNKI told you soUNK', 'expression of pain', '1 expression of disapproval', '1 yes answer to a call', 'is that so expression of uncertain surprise', 'answer in agreement', 'expression of lack of confidence in someoneUNKs ability to perform a task', 'expression of total despair', 'infertile woman ', 'well then', '1 expression of despair over a repeated mishap', '1 exclamation of satisfaction or commendation', '1 palm tree 2 palm fruit', 'palm fruit', 'palm wine', 'palm oil', 'palmfrond broom', 'palm tree', 'which ones nonhuman', 'Africa', 'African'), ('Word', 'a', 'a', 'a', 'a', 'a', 'a', 'aa', 'aa', 'aa', 'aae', 'a-a', 'ãa', 'ãaa', 'âaa', 'aa-ee', 'aa-hii', 'aane', 'aaŋ', 'aba', 'abaa', 'abɛ', 'abɛbiri', 'abɛdãã', 'abɛkãã', 'abɛsaare', 'abɛteɛ', 'abuobo', 'Afereka', 'Aferekaneɛ')]\n","[('preface of a book', '1 to dance 2 to jump up and down', '1 to fly 2 erect', 'to ask for permission to join someone knock door', 'a type of silky', 'bad luck', 'thatUNKs right expression of agreement', 'an expression that denotes UNKwellUNK', 'no', 'an expression of despair', 'expression of boastful challenge', 'expression of disappointment at failure', 'expression of disgust and disagreement', 'expression of despair', 'The thieves have robbed my friend again', 'emphatic form of aku', 'expression indicating oneUNKs inability to do a task', 'God', 'I swear', 'in the name of God', 'to open ring', 'expression ordering one to get out away with you', 'metaphor literature', 'expression of surprise mixed with fear', 'hello', 'aeroplane', 'airport', 'but', 'these 3rd person plural demonstrative nonhuman', 'a type of small tree with large leaves and pink fruit'), ('afu', 'age', 'age', 'agoo', 'agoo', 'agye', 'ahâa', 'ahaŋ', 'ai', 'aii', 'akaase', 'akaasemanakei', 'akaase akai', 'aku', 'akuroku', 'akuroku', 'akuu', 'Ala', 'ala', 'alaakosebaroo', 'ale', 'alee', 'aleɛma', 'alɛ', 'aloo', 'aloopelee', 'aloopeleeduoraa', 'ama', 'ama', 'ambaŋenaa')]\n","[('America', '1 perhaps', 'much more than exaggeration', 'thanks', 'those 3rd person plural strong form nonhuman', 'four', 'because', 'reason', 'those ones 3rd person plural demonstrative', 'it a problem', '1 world 2 people of the world', '1 and 2 although', 'despite', 'effort', 'welcome an expression of goodwill to a stranger who has just arrived', 'gratitude', 'eight', '1 a traditional womenUNKs dance 2 music for such a dance', 'who which person interrogative pronoun', 'before', '1 a type of edible berry that turns violet when ripe 2 the tree of this fruit', 'a woman who has recently given birth', 'noon used as a greeting', 'five', 'one who is great or strong physically or spiritually', 'to be alerted used to caution or persuade', 'who interrogative pronoun', 'no one retort implies the matter is none of the askerUNKs business', 'like', 'having the blades burnt away'), ('Ameleka', 'aminekaŋa', 'amma', 'ammeseɛrɛ', 'ana', 'anaare', 'ananso', 'ananso', 'anaŋ', 'anaŋ', 'andonɛɛ', 'ane', 'aneazaa', 'aneɛ', 'anesɛ', 'angyoɔso', 'anii', 'anlee', 'annoo', 'ansaŋ', 'ansiŋinee', 'ansoɔnee', 'antere', 'anuu', 'anwoŋ', 'aŋ', 'aŋ', 'aŋa', 'aŋa', 'aŋgaara')]\n","[('a type of long grass used to weave sleeping mats', 'to adorn', 'Accra', 'river blindness', 'barrel', 'of course expression of certainty', 'expression of unpleasant surprise', 'emphatic expression of despair', 'to gather up oneUNKs courage', 'kidney', 'to cut', 'to be related through the motherUNKs line', 'to tear off from the main body branch or limb of an animal', 'maternal uncle', 'eldest niece', 'nephew or niece', 'arithmetic', '1 standing place', '1 standing place position 2', 'not negative particle', '1 they', 'to fix a pointed object firmly in the ground', 'to gallop', '1 father 2 friend between males 3 Mister', 'or normally used in question tag', 'dog', 'a small black ground antlike creature which burrows into the ground with the tip of its abdomen', 'a type of childrenUNKs game', 'body of water', '1 to grow up'), ('aŋgeli', 'aŋgoɔle', 'Aŋkara', 'aŋko', 'aŋkorɔ', 'ão', 'apa', 'aparapa', 'ara', 'arambiri', 'are', 'are', 'are', 'areba', 'arekpoŋ', 'arelee', 'aremateke', 'arezie', 'aroozie', 'ba', 'ba', 'ba', 'ba', 'ba', 'baa', 'baa', 'baa', 'baa', 'baa', 'baa')]\n","[('father familiar term of address used by toddlers', 'Bible', 'bustard', 'goodbye', 'bag', 'patient', '1 slim', 'to ache slightly disconfort of the stomach UNK Ka maaUNK iri ba gaa bangyeraa n poUNK maUNK baala la If I donUNKt go to the toilet for the day I normally have a slight stomachache baalUNKUNK', 'to separate grains from unwanted material UNK A pUNKge baale iri la a seUNKkUNKabie yi a pUNKgere poUNK The woman separated the groundnuts from the shells baalUNKUNK', 'hospital', 'the act of supporting someone by holding them by the midsection UNK Ba kpUNK la a baala baalimbo gaa ne a asibiti They supported the patient by the midsection and took him to the hospital', 'germ', 'feast organised for a friend UNK Te daare gaa la baalomboUNKlaa Loraa poUNK We attended a friendship feast in Lawra', 'contagious disease', 'recurring illness', 'friendship UNK Ba nyUNKge la baaloUNK saUNKa zaa They have been friends for a long time', 'illness', '1 calm UNK ZenUNK', '1 gallon measure 2 balloon', 'Ursa Major constellation also known as Big Dipper', 'lily pads pl sg baapUNKmpUNKloo pl baapUNKmpUNKlUNK 2pl baapUNKmpUNKUNKbaapUNKmpUNKllUNKUNK', 'end', '1 to finish', '1 a seed from a type of tree 2 a type of large bead made from this seed', 'Baasare ethnic group', 'bicycle', 'a type of tree that resembles the shea nut tree', 'ignorance', 'one who is innocent', 'innocence'), ('baabaa', 'Baabol', 'bãabõo', 'baaebaae', 'baage', 'baala', 'baalaa', 'baale', 'baale', 'baalebayiri', 'baalimbo', 'baalombiri', 'baalomboɔlaa', 'baalonlɔnnaa', 'baalonnooraa', 'baaloŋ', 'baaloŋ', 'baaloŋ', 'baaluu', 'baa-ne-ŋmaaŋa', 'baapɛmpɛle', 'baaraa', 'baare', 'baasaabiri', 'Baasaale', 'baasakuuri', 'baataŋaa', 'babammo', 'babaŋena', 'babaŋyeli')]\n"]}]},{"cell_type":"code","source":["from torch import nn\n","\n","criterian = nn.CrossEntropyLoss(ignore_index=dagaare_to_index[PADDING_TOKEN],\n","                                reduction='none')\n","\n","# When computing the loss, we are ignoring cases when the label is the padding token\n","for params in transformer.parameters():\n","    if params.dim() > 1:\n","        nn.init.xavier_uniform_(params)\n","\n","optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"VWS4ES3v5Bf1","executionInfo":{"status":"ok","timestamp":1716217664430,"user_tz":0,"elapsed":11,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":202,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def clean_data(sentence):\n","    # Implement data cleaning logic here (e.g., removing special characters, lowercasing)\n","    cleaned_sentence = sentence.lower().strip()  # Example: Lowercase and strip whitespace\n","    return cleaned_sentence\n","\n","def validate_input(x, y, self_attention_mask, cross_attention_mask):\n","    # Check if x and y are tuples and self_attention_mask, cross_attention_mask are torch tensors\n","    if not isinstance(x, tuple) or not isinstance(y, tuple) \\\n","            or not isinstance(self_attention_mask, torch.Tensor) or not isinstance(cross_attention_mask, torch.Tensor):\n","        raise ValueError(\"Input data format is invalid.\")\n","\n","    # Check the shapes of input tensors\n","    if x[0].shape[0] != y[0].shape[0] or x[0].shape[0] != self_attention_mask.shape[0] \\\n","            or x[0].shape[0] != cross_attention_mask.shape[0]:\n","        raise ValueError(\"Batch size mismatch between input tensors and masks.\")\n","\n","    # Add more validation logic if needed\n","\n","def preprocess_input(x, y, self_attention_mask, cross_attention_mask):\n","    # Clean input sentences\n","    x_cleaned = tuple(clean_data(sentence) for sentence in x)\n","    y_cleaned = tuple(clean_data(sentence) for sentence in y)\n","\n","    # Validate input tensors and masks\n","    validate_input(x_cleaned, y_cleaned, self_attention_mask, cross_attention_mask)\n","\n","    return x_cleaned, y_cleaned\n","\n","# Example usage:\n","def forward_transformer(transformer, x, y, self_attention_mask, cross_attention_mask):\n","    # Preprocess input data\n","    x_processed, y_processed = preprocess_input(x, y, self_attention_mask, cross_attention_mask)\n","\n","    # Forward pass through the transformer\n","    predictions = transformer(x_processed, y_processed, self_attention_mask, cross_attention_mask)\n","\n","    return predictions\n"],"metadata":{"id":"5c0L2j--l3r7","executionInfo":{"status":"ok","timestamp":1716218466323,"user_tz":0,"elapsed":435,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":204,"outputs":[]},{"cell_type":"code","source":["transformer.train()\n","transformer.to(device)\n","max_sequence_length= 200\n","total_loss = 0\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch}\")\n","    iterator = iter(train_loader)\n","    for batch_num, batch in enumerate(iterator):\n","        transformer.train()\n","        eng_batch, dg_batch = batch\n","        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, dg_batch,max_sequence_length)\n","        optim.zero_grad()\n","        dg_predictions = transformer(eng_batch,\n","                                     dg_batch,\n","                                     encoder_self_attention_mask.to(device),\n","                                     decoder_self_attention_mask.to(device),\n","                                     decoder_cross_attention_mask.to(device),\n","                                     enc_start_token=False,\n","                                     enc_end_token=False,\n","                                     dec_start_token=True,\n","                                     dec_end_token=True)\n","        labels = transformer.decoder.sentence_embedding.batch_tokenize(dg_batch, start_token=False, end_token=True)\n","        loss = criterian(\n","            dg_predictions.view(-1, dg_vocab_size).to(device),\n","            labels.view(-1).to(device)\n","        ).to(device)\n","        valid_indicies = torch.where(labels.view(-1) == dagaare_to_index[PADDING_TOKEN], False, True)\n","        loss = loss.sum() / valid_indicies.sum()\n","        loss.backward()\n","        optim.step()\n","        #train_losses.append(loss.item())\n","        if batch_num % 100 == 0:\n","            print(f\"Iteration {batch_num} : {loss.item()}\")\n","            print(f\"English: {eng_batch[0]}\")\n","            print(f\"Dagaare Translation: {dg_batch[0]}\")\n","            dg_sentence_predicted = torch.argmax(dg_predictions[0], axis=1)\n","            predicted_sentence = \"\"\n","            for idx in dg_sentence_predicted:\n","              if idx == dagaare_to_index[END_TOKEN]:\n","                break\n","              predicted_sentence += index_to_dagaare[idx.item()]\n","            print(f\"Dagaare Prediction: {predicted_sentence}\")\n","\n","\n","            transformer.eval()\n","            dg_sentence = (\"\",)\n","            eng_sentence = (\"should we go to the mall?\",)\n","            for word_counter in range(max_sequence_length):\n","                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, dg_batch,max_sequence_length)\n","                predictions = transformer(eng_sentence,\n","                                          dg_sentence,\n","                                          encoder_self_attention_mask.to(device),\n","                                          decoder_self_attention_mask.to(device),\n","                                          decoder_cross_attention_mask.to(device),\n","                                          enc_start_token=False,\n","                                          enc_end_token=False,\n","                                          dec_start_token=True,\n","                                          dec_end_token=False)\n","                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n","                next_token_index = torch.argmax(next_token_prob_distribution).item()\n","                next_token = index_to_dagaare[next_token_index]\n","                dg_sentence = (dg_sentence[0] + next_token, )\n","                if next_token == END_TOKEN:\n","                  break\n","\n","            print(f\"Evaluation translation (should we go to the mall?) : {dg_sentence}\")\n","            print(\"-------------------------------------------\")"],"metadata":{"id":"_NuMA-Jk7Cld","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1716218477699,"user_tz":0,"elapsed":7911,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}},"outputId":"5a080125-0923-4d96-c9de-29cc8c5d36a1"},"execution_count":205,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0\n","encoder_self_attention_mask torch.Size([30, 200, 200]): tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","decoder_self_attention_mask torch.Size([30, 200, 200]): tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n","decoder_cross_attention_mask torch.Size([30, 200, 200]): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n","Contents of x: ('Definition', '1 and', '2 in order to', 'but', 'they', 'the', 'to hate', 'expression of surprise and regret', 'expression of regret', 'expression expressing UNKI told you soUNK', 'expression of pain', '1 expression of disapproval', '1 yes answer to a call', 'is that so expression of uncertain surprise', 'answer in agreement', 'expression of lack of confidence in someoneUNKs ability to perform a task', 'expression of total despair', 'infertile woman ', 'well then', '1 expression of despair over a repeated mishap', '1 exclamation of satisfaction or commendation', '1 palm tree 2 palm fruit', 'palm fruit', 'palm wine', 'palm oil', 'palmfrond broom', 'palm tree', 'which ones nonhuman', 'Africa', 'African')\n","Contents of y: ('Word', 'a', 'a', 'a', 'a', 'a', 'a', 'aa', 'aa', 'aa', 'aae', 'a-a', 'ãa', 'ãaa', 'âaa', 'aa-ee', 'aa-hii', 'aane', 'aaŋ', 'aba', 'abaa', 'abɛ', 'abɛbiri', 'abɛdãã', 'abɛkãã', 'abɛsaare', 'abɛteɛ', 'abuobo', 'Afereka', 'Aferekaneɛ')\n","Type of x: <class 'tuple'>\n","Type of y: <class 'tuple'>\n","Iteration 0 : 3.050119638442993\n","English: Definition\n","Dagaare Translation: Word\n","Dagaare Prediction: aaaa\n","encoder_self_attention_mask torch.Size([30, 200, 200]): tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","decoder_self_attention_mask torch.Size([30, 200, 200]): tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n","decoder_cross_attention_mask torch.Size([30, 200, 200]): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n","        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n","         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n","Contents of x: ('should we go to the mall?',)\n","Contents of y: ('',)\n","Type of x: <class 'tuple'>\n","Type of y: <class 'tuple'>\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"shape '[1, 200, 512]' is invalid for input of size 3072000","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-205-fa21871cade3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword_counter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 predictions = transformer(eng_sentence,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                           \u001b[0mdg_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                           \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dagaare_words_definition/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[1;32m    306\u001b[0m                            \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                            \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                            \u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m                            \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_start_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                            end_token=dec_end_token)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dagaare_words_definition/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dagaare_words_definition/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dagaare_words_definition/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attention_mask)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mresidual_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dagaare_words_definition/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 200, 512]' is invalid for input of size 3072000"]}]},{"cell_type":"code","source":["# Inside the forward method of DecoderLayer class\n","def forward(self, x, y, self_attention_mask, cross_attention_mask):\n","    print(\"Input x shape:\", x.shape)\n","    print(\"Input y shape:\", y.shape)\n","    print(\"Self-attention mask shape:\", self_attention_mask.shape)\n","    print(\"Cross-attention mask shape:\", cross_attention_mask.shape)\n","\n","    _y = y.clone()\n","    y = self.self_attention(y, mask=self_attention_mask)\n","    print(\"Output of self-attention shape:\", y.shape)\n","\n","    # Continue with other layers and print intermediate outputs\n","\n","    return y\n"],"metadata":{"id":"qt_cfSWlmxlz","executionInfo":{"status":"aborted","timestamp":1716217671833,"user_tz":0,"elapsed":26,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translate(eng_sentence, max_sequence_length=100):\n","    eng_sentence = [tokenizer.encode(eng_sentence, add_special_tokens=False)]\n","    dg_sentence = [[tokenizer.cls_token_id]]  # Start with the start token\n","\n","    for word_counter in range(max_sequence_length):\n","        eng_tensor = torch.tensor(eng_sentence).to(device)\n","\n","        if dg_sentence[-1][-1] == tokenizer.sep_token_id:\n","            break\n","\n","        dg_tensor = torch.tensor(dg_sentence).to(device)\n","\n","        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(\n","            eng_tensor, dg_tensor, max_sequence_length\n","        )\n","\n","        predictions = transformer(\n","            eng_tensor,\n","            dg_tensor,\n","            encoder_self_attention_mask.to(device),\n","            decoder_self_attention_mask.to(device),\n","            decoder_cross_attention_mask.to(device),\n","            enc_start_token=False,\n","            enc_end_token=False,\n","            dec_start_token=True,\n","            dec_end_token=False\n","        )\n","\n","        next_token_prob_distribution = predictions[0][word_counter]\n","        next_token_index = torch.argmax(next_token_prob_distribution).item()\n","\n","        # Add next token index to dg_sentence if it's not None\n","        if next_token_index is not None:\n","            dg_sentence[0].append(next_token_index)\n","\n","    # Decode dg_sentence if it's not empty\n","    if dg_sentence and dg_sentence[0]:\n","        return tokenizer.decode(dg_sentence[0], skip_special_tokens=True)\n","    else:\n","        return \"\"  # Return empty string if dg_sentence is empty\n","\n","# Example usage\n","translation = translate(\"what is your name?\", max_sequence_length=100)\n","print(translation)\n"],"metadata":{"id":"o4nin0RZ7gIJ","executionInfo":{"status":"aborted","timestamp":1716217671833,"user_tz":0,"elapsed":26,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","def compute_bleu(reference, candidate):\n","    reference = [reference.split()]\n","    candidate = candidate.split()\n","    smoothie = SmoothingFunction().method4\n","    score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n","    return score\n","\n","# Example usage\n","reference_sentence = \"What is your name?\"\n","candidate_sentence = translate(\"What is your name?\", max_sequence_length=100)\n","bleu_score = compute_bleu(reference_sentence, candidate_sentence)\n","print(f\"BLEU Score: {bleu_score:.2f}\")\n"],"metadata":{"id":"W-8E6Xx-kWR_","executionInfo":{"status":"aborted","timestamp":1716217671833,"user_tz":0,"elapsed":26,"user":{"displayName":"Leslie Nouchie","userId":"02622351220458933444"}}},"execution_count":null,"outputs":[]}]}